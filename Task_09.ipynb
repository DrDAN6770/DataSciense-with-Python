{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "o1a1E2-cYMBS",
        "g5TYAK6jZVau"
      ],
      "authorship_tag": "ABX9TyNFOVczkYVK5CcpiuJVCDYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrDAN6770/DataSciense-with-Python/blob/Python-%E7%A8%8B%E5%BC%8F%E8%A8%AD%E8%A8%88%E5%9F%BA%E7%A4%8E%E9%A4%8A%E6%88%90_%E5%AF%A6%E6%88%B0-%E7%A4%BE%E7%BE%A4%E5%85%B1%E5%AD%B8/Task_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 作業#01 【實作題】新聞語料關鍵字分析"
      ],
      "metadata": {
        "id": "WoyePX5jXa0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1 斷詞（tokenize）\n",
        "\n",
        "---\n",
        "所謂的斷詞（tokenize）是指將文本中的「單字」萃取而出的動作，請問中文字和英文字在斷詞處理上可能有哪些差異存在呢？\n"
      ],
      "metadata": {
        "id": "iw1m8li2gmDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "英文就是單字和單字之間\n",
        "\n",
        "中文大概是詞性吧，所以不太容易處理"
      ],
      "metadata": {
        "id": "0HYFXY0yguCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2\n",
        "\n",
        "---\n",
        "\n",
        "第一個步驟請利用 聯合新聞網 作為主要的文本來源，將範例收集回來的新聞內文存在一個 List 容器變數 `news` 內"
      ],
      "metadata": {
        "id": "ofF6ViNTg_Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "0W3C2NSehLyG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 利用 requests 對 API 來源發送一個請求\n",
        "url = 'https://udn.com/news/breaknews/1'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text)\n",
        "\n",
        "news = []\n",
        "\n",
        "for link in soup.find_all('h3', class_='rounded-thumb__title')[:4]:\n",
        "  news_url = link.a['href']\n",
        "  news_response = requests.get('https://udn.com' + news_url)\n",
        "  news_soup = BeautifulSoup(news_response.text)\n",
        "  news_content = news_soup.find('div', class_='article-content__paragraph').text.strip().replace('\\n', ' ')\n",
        "  \n",
        "  #print(news_content)\n",
        "  news.append(news_content)\n",
        "print(news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZPjzgvbhJ8V",
        "outputId": "beded835-f349-47c6-de7b-fbb83786d6af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['最近天氣越來越熱，許多人已將厚重衣物收起，換上輕薄舒適的穿搭。然而，就有一名男大生PO文表示，近日逛夜市時發現，不少女生都會穿著很透很薄的白長褲，有時還無法完全遮住內褲顏色，因此好奇發問「我還是不太懂為什麼會想這樣穿？」掀起網友熱議。 \\r 該名男大生在《Dcard》發文表示，前幾天逛夜市發現，一堆女生都會穿著很透很薄的白長褲，「輕則露出內褲痕，嚴重一點我連她穿什麼樣式顏色的內褲都看得出來」，讓他不禁疑惑問道「女生都喜歡穿透明白長褲？我還是不太懂為什麼會想這樣穿？」            \\r 貼文一出，引發網友熱烈討論，紛紛留言「確實三不五時會看到這種的，我也真的很納悶為什麼要這樣」、「我是女生，我也不懂」、「之前也有看過，還在我面前彎腰，內褲看得一清二楚」、「因為通常大家穿完褲子是在家，光沒那麼強烈，所以照鏡子時不一定會發現褲子很透」、「我覺得有時候室內外看起來會不太一樣，覺得很多人不是刻意的，是買到品質不好太薄的」。 \\r 此外，還有網友提到「她不知道，結案。之前我們教授也這樣穿，我還去提醒她，她根本不知道內褲花色看得一清二楚」、「我覺得是因為自己看不到後面，所以才不小心穿出門的…我上次也提醒我同學，而且我也超怕自己這樣，超尷尬」、「所以我買白長褲都會特別注意，很多內褲痕真的好明顯，但想要那種垂墜的型，布料通常又薄又貼膚，所以一定很明顯」、「我就是那種女生，覺得褲子好看就不在意」。    女生 Dcard', '新北市有民眾發現一名男子在前天白天跪在林口交流道，舉著牌子「媽媽住院請幫忙」，引人矚目，警察到場後，男子卻轉頭就跑，連牌子也不要了，林口區公所社會課發現，這名男子住在桃園，家中母親並沒有狀況，男子已3年沒跟家裡連繫，也不願多談細節並拒絕協助。 \\r 一名網友在臉書社團「林口大家庭LinKou-Family」PO出照片，發現一名男子跪在林口南下交流道的道路旁，還舉著保麗龍板「媽媽住院請幫忙」，看似尋求援助，引來周邊民眾關注。            \\r 不過，林口分局、新北市議員蔡淑君獲報後，都前往現場關心。不過這名男子一看到警察就丟掉牌子，轉頭想跑，警方趕緊上前關心，希望了解詳情。 \\r 林口區長廖武輝指出，林口分局員警、林口區公所詢問男子狀況時，他只願意提供身分證字號及姓名，警方希望能幫忙，但男子都拒絕提供其他相關資料，也拒絕幫忙。後來發現男子是桃園中壢人，就將他轉介至桃園市社福機構。 \\r 廖武輝說，桃園方經了解後發現，這名男子家庭狀況並沒有問題，母親安在，男子也不是低收入戶，警方直接與他接觸時，也沒有精神異常的感覺，目前只能積極聯繫當事人，了解他是否需要協助。 \\r 新北市議員蔡淑君則說，該男子當天在國道1號林口區南下匝道口舉牌下跪，但警方到現場後，他又將牌子丟掉，拒絕協助。後續警方詢問，確認是桃園市的民眾，自稱從事殯葬業，但經濟狀況不穩定，加上媽媽長期住院因此才出來舉牌。不過據了解，該男子疑似有前科，且鄰居說他已經很久沒跟家裡聯絡，媽媽也沒有住院。一名男子在林口交流道舉牌「媽媽住院請幫忙」，然而警察想上前協助，男子卻轉頭就跑，經查，該男子家中並無狀況。圖／擷自臉書社團「林口大家庭LinKou-Family」    林口 桃園', '在國外容易遇到語言不通的問題，最好的方法是先確認過後再行動。有一名網友透露，最近和朋友在東京吃烏龍麵時，看到日文「玉」直覺是雞蛋，結果和友人各點了三顆，心想不用加價好划算，結果送來時悲劇了。 \\r 網友在臉書社團「爆廢公社」發文表示，最近和朋友到東京玩，晚餐吃烏龍麵，結果看到菜單上寫著「3つ玉」，腦袋一閃「玉就是雞蛋嘛這題我會」，心想著加雞蛋竟然免錢，於是和朋友都點了「3つ玉」。            \\r 沒想到烏龍麵送來的時候他們傻眼，因為日文的「3つ玉」其實是三坨麵的意思，結果份量多到和朋友都吃不完，「我還想說為什麼我們吃的這麼辛苦」，原po也自嘲「抱歉我今天是失格旅人，我不該被生出來」，他也驚訝隔壁的日本人食量驚人，都吃光光。 \\r 網友也紛紛留言回應，「玉子才是雞蛋」、「學一半，糗了！」、「這事我也做過，好在是加了一坨麵」、「沒事，我四年前在九州outlet也做過，兩碗大盛加三玉以為是加蛋這種事，店員一再跟我確認，我只會OKOK 3q3q，結果麵來了我自己都嚇到」、「不好意思我笑了，而且免費的沒吃完是很失禮的事啊」。     東京', '湖南省委書記沈曉明日前在會晤前總統馬英九時提到，長沙的夜市非常熱鬧，和北京上海相比是「大城市」。馬前總統二日晚，帶著大九學堂學生逛太平街夜市，沿途民眾紛紛高喊「歡迎馬先生回鄉！」「歡迎你來，多做交流」「馬先生多回老家看看！」 \\r 太平街坐落於長沙市老城區南部，是一條適合逛逛吃吃的老街，能吃到地道的長沙臭豆腐、糖油粑粑等當地小吃。 同時，太平街也是一個有文化底蘊的街道，有賈誼故居、長懷井、明吉藩王府西牌樓等古蹟，還有一些青瓦白牆的長沙老民居。            \\r 馬英九一行是晚間七時卅分左右，在層層保安的保護下抵達太平老街，一路上並沒有「清空」民眾，兩旁民眾紛紛拿起手機拍照，有的民眾看見馬英九一行鼓起掌來，馬英九也微笑揮手致意，甚至用長沙話向老鄉們問好，氣氛熱絡。  路程走到一半，看到長沙著名的手搖飲料店「茶顏悅色」，有排隊的民眾看到是馬英九來了，乾脆不排了，紛紛圍過來和馬英九握手，馬英九也一一和鄉親們握手，行進的速度非常緩慢，讓隨扈非常緊張。 \\r 據悉，太平街平時就是人擠人的夜市，恰逢周日人潮更為擁擠，馬英九一行到來，圍觀的人多，使人群更顯得擁擠，簡直寸步難行。 \\r 除了大馬路有維安清空外，夜市本身並沒有清空，民眾看到馬英九十分激動，大喊「馬先生好！」「馬先生歡迎回來！」「希望以後多多回老家來！」      馬英九 夜市 老街']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#文本資料處理"
      ],
      "metadata": {
        "id": "vZ4-BU5SXhiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "資料庫Python 提供了許多用於處理文本的庫和模組。以下是一些常用的操作與工具：\n",
        "\n",
        "\n",
        "* 正則表達式 re\n",
        "* Natural Language Toolkit(NLTK)\n",
        "* Jieba\n",
        "\n"
      ],
      "metadata": {
        "id": "W7KZ48YZXnbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 正則表達式\n",
        "\n",
        "---\n",
        "\n",
        "正則表達式是一種強大的文本處理工具，可用於搜索、替換和提取文本。Python 內置了 re 函式庫，用於處理正則表達式"
      ],
      "metadata": {
        "id": "o1a1E2-cYMBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog.'\n",
        "result = re.search(r'fox', text)\n",
        "print(result.group(0)) # fox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv2M19VFYK_E",
        "outputId": "43f12f60-4681-499d-ccc7-619f71d3b857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"John Doe, jane.doe@example.com\"\n",
        "pattern = r\"(\\w+) (\\w+), (\\w+.\\w+@\\w+\\.\\w+)\"\n",
        "\n",
        "match = re.search(pattern, text)\n",
        "\n",
        "if match:\n",
        "  print(match.group(1))   # Output: John\n",
        "  print(match.group(2))   # Output: Doe\n",
        "  print(match.group(3))   # Output: jane.doe@example.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoDo5A0YZL-t",
        "outputId": "3190cb5f-67ab-4137-8f80-eba919b7bb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John\n",
            "Doe\n",
            "jane.doe@example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK\n",
        "\n",
        "---\n",
        "\n",
        "提供了許多自然語言處理的工具和資源。它可以用於分詞、標記、詞性標注和文本分析。"
      ],
      "metadata": {
        "id": "g5TYAK6jZVau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9_mg524hZcos",
        "outputId": "b2d209f1-63f4-4fac-e1b2-0cecd511af8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog.'\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens) # ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0t4fYGOanew",
        "outputId": "02203189-3738-41fb-9b7e-5f4895f07938"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in text.split(\" \")] #差在最後的那個句點"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDPekfqHarrf",
        "outputId": "4f3739f2-4187-4672-9590-c4d2b83402f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Jieba\n",
        "\n",
        "---\n",
        "\n",
        "對於中文的自然語言處理，Jieba 是一個流行的 Python 函式庫。Jieba 提供了分詞、詞性標注、關鍵詞提取等功能，是進行中文文本處理的好幫手。"
      ],
      "metadata": {
        "id": "sJhCLMBXbLoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1)將中文文本分成詞語，可以使用 Jieba 的 cut 方法"
      ],
      "metadata": {
        "id": "4jsa1GEebWUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba"
      ],
      "metadata": {
        "id": "Q5V-zKqAbSNp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '我喜歡用Python編程'\n",
        "words = jieba.cut(text) #返回的是一个 generator，即生成器\n",
        "print('/'.join(words)) # 我喜/歡用/Python/編程"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkuEA7WbkFH",
        "outputId": "5da42a77-24ae-44b0-8984-b039d5ba3209"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我喜/歡用/Python/編程\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(2) 對中文文本進行詞性標注，可以使用 Jieba 的 posseg 方法。"
      ],
      "metadata": {
        "id": "ufsSGok3dYKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba.posseg as pseg\n",
        "\n",
        "text = '我喜歡用Python編程'\n",
        "words = pseg.cut(text)\n",
        "for word, pos in words:\n",
        "    print(word, pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eBDX--odbQ3",
        "outputId": "fbf8b3be-b1c1-4af0-c79f-77fe4e7d01a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'generator'>\n",
            "我 r\n",
            "喜歡 v\n",
            "用 p\n",
            "Python eng\n",
            "編程 n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(3) 提取中文文本的關鍵詞，可以使用 Jieba 的 extract_tags 方法"
      ],
      "metadata": {
        "id": "aj_Uh3gcduRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba.analyse\n",
        "\n",
        "text = '我喜歡用Python編程'\n",
        "keywords = jieba.analyse.extract_tags(text, topK=3)\n",
        "print(keywords) # ['Python', '喜歡', '編程']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPPpKQkxdvOc",
        "outputId": "3eb5c6ac-53cc-4804-9343-634d8ac933b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['我喜', '歡用', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 自然語言處理\n",
        "\n",
        "---\n",
        "在自然語言處理中，您可以使用 Python 來進行各種文本分析、語言模型、情感分析等工作。\n"
      ],
      "metadata": {
        "id": "Xkqt3g9tX-ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TextBlob\n",
        "\n",
        "---\n",
        "TextBlob 是一個基於 NLTK 的 Python 库，提供了一個簡單的 API，用於文本分析、情感分析和主題分析。\n"
      ],
      "metadata": {
        "id": "j0bKdMS1d1dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = 'I love Python!'\n",
        "blob = TextBlob(text)\n",
        "print(blob.sentiment.polarity) # 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtjCgNLyd8Xg",
        "outputId": "36b8ae3e-f835-4b41-a98a-f78a4a17fac2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###gensim\n",
        "\n",
        "---\n",
        "\n",
        "gensim 是一個用於主題建模和相似度比較的 Python 函式庫。它可以用於建立文檔、詞彙和主題模型，以及進行文本\"相似度\"比較"
      ],
      "metadata": {
        "id": "zNGLV0u2eBtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models, similarities\n",
        "\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\", \n",
        "    \"I love Python and machine learning\"\n",
        "]\n",
        "texts = [[word for word in document.lower().split()] for document in documents]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "tfidf = models.TfidfModel(corpus)             #計算corpus中每個單詞的TF-IDF權重(tfidf)\n",
        "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
        "query = \"Python and machine learning\"\n",
        "query_bow = dictionary.doc2bow(query.lower().split())\n",
        "sims = index[tfidf[query_bow]]\n",
        "print(list(enumerate(sims)))\n",
        "# [(0, 0.0), (1, 0.024158917), (2, 0.78141415)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rII0AZFeFaZ",
        "outputId": "7b772b45-f902-460e-d178-d4bb7fefb96d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.0), (1, 0.024158917), (2, 0.78141415)]\n"
          ]
        }
      ]
    }
  ]
}